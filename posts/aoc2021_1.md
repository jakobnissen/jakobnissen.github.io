# Advent of Code 2021, day 1
_Written 2021-12-01_

This year's [Advent of Code](https://adventofcode.com/) has begun, meaning that ♪'tis the season to be coding♪.
AoC is a _great_ opportunity to learn a new programming language or new set of programming tools.
For example, you could use this year's AoC to learn... oh, I don't know, [Julia](https://julialang.org/)?

Great choice! You've come to the right place.
In this post, I will be showing you how I've solved [the day 1 puzzle](https://adventofcode.com/2021/day/1) in Julia.

The day 1 puzzle is not very challenging.
But because this post is _really_ about how best to get started coding a project using Julia, with unit tests and benchmarks and the whole shebang, the solution will be wildly over-engineered.

## Setting up your project
You'll want to [install Julia first](https://julialang.org/downloads/#current_stable_release) (of course), and some kind of editor.
I recommend [VS Code](https://code.visualstudio.com/), because it currently has the best Julia IDE, but editors can be an, uh, [touchy subject](https://en.wikipedia.org/wiki/Editor_war), so you just pick your favourite.
If you choose VS Code you'll also want to install the [julia extension](https://www.julia-vscode.org/) - just get the extension called "julia".

To solve the 24 Advent of Code puzzles, We _could_ just make a series of scripts, but let's do it the _right_ way and instead create a new Julia project for this year's AoC.
We can then have one source file per day.
To do this, launch Julia, then type `]` to enter `Pkg` mode, then type:

@@juliacode
```plaintext
(@v1.6) pkg> generate AoC2021
  Generating  project AoC2021:
    AoC2021/Project.toml
    AoC2021/src/AoC2021.jl```
@@

This is the part where some people grimace in disgust: I'm using _the REPL_ to initialize a project from within Julia? Why not simply do it from the shell?

Right, about that... When developing Julia, it is _possible_ to run all your commands from the shell, but I strongly recommend you don't do that, and work from the REPL instead.
For some people, this may be a jarring shift in your normal development workflow, I know.
But when in Rome, do as the Romans - Julia's development experience really does work better when you interact with it from the REPL. If it makes it easier, you can use the Julia REPL in the VSCode terminal.

With your open REPL, navigate to the `AoC2021` directory you just created (type ';' to enter shell mode), enter package mode and execute `activate .` (note the trailing dot for "this directory"!) to have the package manager activate the project.
If it works, you should see the prompt look like

@@juliacode
```plaintext
(AoC2021) pkg>
```
@@

You can then use backspace to get back to the `julia>` REPL mode. Okay, so currently our directory looks like:

@@shellcode
```plaintext
$ exa -T
.
├── Project.toml
└── src
   └── AoC2021.jl
```
@@

Let's add a `data` directory and add today's input file in that.
We also add a new `day01.jl` file in the `src` directory for today's code, and make the toplevel project file `AoC2021.jl` file `include` the new file.
The directory structure is now:

@@shellcode
```plaintext
$ exa -T
.
├── data
│  └── day01.txt
├── Project.toml
└── src
   ├── AoC2021.jl
   └── day01.jl
```
@@

And the main file `AoC2021.jl` contains:

@@juliacode
```plaintext
module AoC2021

include("day01.jl")

end # module
```
@@

Let's also make the directory a git repository - that's always a good idea when developing:

@@shellcode
```plaintext
$ git init
```
@@

and add the following to `.gitignore`:

@@shellcode
```plaintext
/data
```
@@

We don't want to waste _kilobytes_ by tracking the data directory (and in any case, different people get different challenge inputs).
Normally, when developing Julia packages, we would also ignore the `Manifest.toml` file which we will soon create with the Julia package manager.
See where the `Project.toml` file is used the specify dependencies and compatibility bounds for a package, `Manifest.toml` contains the resolved dependency graph of the project.
If you are familiar with Rust, `Project.toml` and `Manifest.toml` correspond to `Cargo.toml` and `Cargo.lock`, respectively.
The `Manifest.toml` file can be used if the user needs to reconstruct an environment _precisely_, for example when doing reproducible science.
When developing packages, you usually _don't_ want to completely specify an environment.
Instead you want your package to be usable in a broad range of environments.
And so, normally, the manifest is ignored with version control.

However, in our case, an Advent of Code challenge is one of those cases where you might want to have the full environment, so here, we keep the manifest.

Now, we can begin to look at today's challenge. The structure of the file `day01.jl` will be like this:

@@juliacode
```plaintext
module day01

function solve(io::IO)
	# code goes here...
end

end # module
```
@@

Such that the function can accept any IO-like argument - this could be a string, or a file-like object.
To create more generic code, I could also just have left off the type annotation `::IO` altogether, but we really do only want to call it on IO inputs, and the type annotation can serve as self-documenting code.

## The code itself
So, for day 1, we have an input that looks like this:

@@shellcode
```plaintext
1721
979
366
299
675
1456
```
@@

And we need to find the two elements that sum to 2020, and multiply them.

## Adding tests
Of course, no project is complete without tests. We could use [Julia's built-in testing package](https://docs.julialang.org/en/v1/stdlib/Test/), but that is quite bare-bones. Instead, let us use `ReTest`, which allows us to write tests next to the source code the tests work on (and much more). To add `ReTest` as a dependency, simply use the package manager:

 @@juliacode
 ```plaintext
(AoC2021) pkg> add ReTest@0.3
```
@@

I specify version 0.3 simply to make sure the code in this post will remain functional in the future.
Feel free to install the latest version.
We import the package in the top-level file `AoC2021.jl`:

@@juliacode
```plaintext
module AoC2021

using ReTest

include("day01.jl")

end # module
```
@@

and also import it at the top of the `day01` file using a relative import (so it imports it from the top-level module):

@@juliacode
```plaintext
module day01

using ..ReTest

[ rest of file elided ]
```
@@

We now add some tests near the bottom of the `day01.jl` file.
AoC helpfully provides us with a small test example in the project description. Let's make that a constant in the code:

@@juliacode
```plaintext
const TEST_STRING = """1721
979
366
299
675
1456"""
```
@@

We can now write a test set using the `@testset` macro, that makes sure our code works for the test data at least:

@@juliacode
```plaintext
@testset "sum_elements" begin
	@test sum_elements([1, 2, 4], 5) == (1, 4)
	@test sum_elements([0x02, 0x04, 0x07], 0x0b) === (0x04, 0x07)
	@test sum_elements([1], 1) === nothing
	@test sum_elements([6, 4, 1], 8) === nothing
	@test sum_elements(Int[], 0) === nothing

	numbers = [parse(Int, line) for line in eachline(IOBuffer(TEST_STRING))]
	@test sum_elements(numbers, 2020) == (1721, 299)
end
```
@@

To test it, we import our module `AoC2021`, and run `AoC2021.runtests()`:

@@juliacode
```plaintext
julia> using AoC2021

julia> AoC2021.runtests()
                       Pass  
Main.AoC2021.day01:
  sum_elements     |      6
```
@@

## Adding static analysis
After having tested, we can be fairly sure it works as we expect. However, writing a comprehensive test suite that covers all the edge cases is difficult.
To be a little more confident our program is well-behaved, we can analyze the behaviour statically.
Of course for projects as tiny as one Advent of Code day, there really isn't much point to static analysis, but let me show you anyway:

For this, I use the package `JET.jl`.
We don't need it as part of the `AoC2021` project, so I install the package to the default environment instead:

@@juliacode
```plaintext
(AoC2021) pkg> activate
  Activating environment at `~/.julia/environments/v1.6/Project.toml`

(@v1.6) pkg> add JET
  [ output elided ]

(@v1.6) pkg> activate .
  Activating environment at `~/code/AoC2021/Project.toml`
```
@@

An interesting wrinkle when doing static analysis of Julia is that the program's behaviour is essentially un-analyzable until it is actually compiled, and it is not compiled until we run it - or at least give concrete input types to its functions.
Therefore, unlike for static languages, it is mostly meaningless to try to analyze the _source code_ of our file - instead, we have to analyze specific _uses_ of the code.

Here, for example, I analyze on the type-level how the main function behaves when called with an `IOBuffer`:

@@juliacode
```plaintext
julia> using JET

julia> @report_call AoC2021.day01.solve(IOBuffer())
No errors !
Union{Nothing, Int64}
```
@@

No errors! Whew!

It would be possible to add `JET.jl` static analysis to the test suite.
However, with the current state of static analysis in Julia, this is not advisable.
Unlike static languages, the presence of a _potential_ error - for example an unresolvable function call - may not be an actual problem in Julia code.
For example, suppose we had a few dependencies, and the dependencies got updated.
It would be completely fair game for these dependencies to now include code that is not statically inferrable, but which behaves correctly when running.
It is therefore best to not include static analysis in automatic tests. Static analysis may be improved in future versions of Julia, but for now, we'll leave it as it is.

## Benchmarking
Wise programmers say that, most of the time, slow code is fast enough, and developers should optimize their code for maintainability instead of speed.
That may be true, but that has never stopped me from over engineering otherwise uncomplicated software until it goes _brrrrrr_.

For benchmarking, we'll use the tested and tried `BenchmarkTools` package.
Again, this shouldn't be a dependency of our actual project, so let's install it in our home environment:

@@juliacode
```plaintext
(AoC2021) pkg> activate
  Activating environment at `~/.julia/environments/v1.6/Project.toml`

(@v1.6) pkg> add BenchmarkTools
  [ output elided ]

(@v1.6) pkg> activate .
  Activating environment at `~/code/AoC2021/Project.toml`
```
@@

Now we can benchmark a function call like so:

@@juliacode
```plaintext
julia> @benchmark open(AoC2021.day01.solve, "data/day01.txt")
BenchmarkTools.Trial: 1609 samples with 1 evaluation.
 Range (min … max):  2.843 ms …   6.682 ms  ┊ GC (min … max): 0.00% … 0.00%
 Time  (median):     2.880 ms               ┊ GC (median):    0.00%
 Time  (mean ± σ):   3.101 ms ± 525.799 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%

  ██                                                        ▃
  ████▇█▇▇▇▇▇▇▆▇▆▆▇▆▇▆▆▆▅▇▅▆▆▆▄▅▆▆▄▆▅▆▅▅▆▄▆▄▆▅▅▅▅▄▅▅▅▅▄▅▄▆▁▆█ █
  2.84 ms      Histogram: log(frequency) by time      4.79 ms <

 Memory estimate: 16.16 KiB, allocs estimate: 420.
```
@@

(or get just the minimum time using `@btime`).
Those times include reading and parsing the input file. Not too unreasonable!

## The easy way
Setting up the whole environment and tests and such was a lot of work.
Does it really have to be that complicated to code in Julia?

No! A more typical Julia solution for day one could simply be a script with:

@@juliacode
```plaintext
function main(path)
    v = [parse(Int, line) for line in eachline(path)]
    for n in (2, 3), i in Iterators.product(v, n)
        sum(i) == 2020 && println(prod(i))
    end
    Iterators.product(v, 3) |> filter
end

main(ARGS[1])
```
@@
