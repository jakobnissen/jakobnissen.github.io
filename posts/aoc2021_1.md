# Advent of Code 2021, day 1
_Written XXXXXXXXXXXXXXX_

This year's [Advent of Code](https://adventofcode.com/) has begun, meaning that ♪'tis the season to be coding♪.
AoC is a _great_ opportunity to learn a new programming language or new set of programming tools.
For example, you could use this year's AoC to learn... oh, I don't know, [Julia](https://julialang.org/)?

Great choice! You've come to the right place. In this post, I will be showing you how I've solved [the day 1 puzzle](https://adventofcode.com/2021/day/1) in Julia.
The day 1 puzzle is not very challenging, so this post is _really_ about how best to get started coding a project using Julia.

## Setting up your project

You'll want to [install Julia first](https://julialang.org/downloads/#current_stable_release) (of course), and some kind of editor.
I recommend [VS Code](https://code.visualstudio.com/), because it currently has the best Julia IDE, but editors can be an, uh, [touchy subject](https://en.wikipedia.org/wiki/Editor_war), so you just pick your favourite.
If you choose VS Code you'll also want to install the [julia extension](https://www.julia-vscode.org/) - just get the extension called "julia".

So, for day 1, we have an input that looks like this:

@@shellcode
```plaintext
1721
979
366
299
675
1456
```
@@

And we need to find the two elements that sum to 2020, and multiply them.

To solve the 24 Advent of Code puzzles, We _could_ just make a series of scripts, but let's do it the _right_ way and instead create a new Julia project for this year's AoC. To do this, launch Julia, then type `]` to enter `Pkg` mode, then enter:

@@juliacode
```plaintext
(@v1.6) pkg> generate AoC2021
  Generating  project AoC2021:
    AoC2021/Project.toml
    AoC2021/src/AoC2021.jl```
@@

This is the part where some people grimace in disgust: I'm using _the REPL_ to initialize a project from within Julia? Why not simply do it from the shell?

Yes, about that... When developing Julia, it is _possible_ to run all your commands from the shell, but I strongly recommend you don't do that, and work from the REPL instead.
For some people, this may be a jarring shift in your normal development workflow, I know. But when in Rome, do as the Romans - Julia's development experience really does work better when you interact with it from the REPL. If it makes it easier, you can use the Julia REPL in the VSCode terminal.

With your open REPL, navigate to the `AoC2021` directory you just created, enter package mode and execute `activate` to activate the project. If it works, you should see the prompt look like

@@juliacode
```plaintext
(AoC2021) pkg>
```
@@

You can then use backspace to get back to the `julia>` REPL mode. Okay, so currently our directory looks like:

@@shellcode
```plaintext
$ exa -T
.
├── Project.toml
└── src
   └── AoC2021.jl
```
@@

Let's add a `data` directory and add today's input file in that, add a new `day1.jl` file in the `src` directory where we put today's code, and make the main `AoC2021.jl` file include the new file. The directory structure is now:

@@shellcode
```plaintext
$ exa -T
.
├── data
│  └── day1.txt
├── Project.toml
└── src
   ├── AoC2021.jl
   └── day1.jl
```
@@

And the main file `AoC2021.jl` contains:

@@juliacode
```plaintext
module AoC2021

include("day1.jl")

end # module
```
@@

Let's also make the directory a git repository - that's always a good idea when developing:

@@shellcode
```plaintext
$ git init
```
@@


and add the following to `.gitignore`:

@@shellcode
```plaintext
/Manifest.toml
/data
```
@@

We don't want to waste _kilobytes_ by tracking the data directory (and in any case, different people get different challenge inputs). We also ignore the `Manifest.toml` file.
It contains the resolved dependency graph of the project, and hence is a machine-generated file used by Julia package manager.
It's useful when you need to _completely_ specify the environment, such as when doing reproducible science, but otherwise it's not meant to be shared between users.
The `Project.toml` should contain the relevant information about compatibilities etc.

Now, we can begin to look at today's challenge. The structure of the file `day1.jl` will be like this:

@@juliacode
```plaintext
module day1

function solution(io::IO)
	# code goes here...
end

end # module
```
@@

Such that the function can accept any IO-like argument. To create more generic code, I could also just have left off the type annotation `::IO` altogether, but we really do only want to call it on IO inputs, and the type annotation can serve as self-documenting code.

## The code itself

## Adding tests
Of course, no project is complete without tests. We could use [Julia's built-in testing package](https://docs.julialang.org/en/v1/stdlib/Test/), but that is quite bare-bones. Instead, let us use `ReTest`, which allows us to write tests next to the source code the tests work on (and much more). To add `ReTest` as a dependency, simply use the package manager:

 @@juliacode
 ```plaintext
(AoC2021) pkg> add ReTest@0.3
```
@@

We import the package in the top-level file `AoC2021.jl`:

@@juliacode
```plaintext
module AoC2021

using ReTest

include("day1.jl")

end # module
```
@@

and also import it at the top of the `day1` file using a relative import (so it imports it from the top-level file):

@@juliacode
```plaintext
module day1

using ..ReTest

[ rest of file elided ]
```
@@

We now add some tests near the bottom of the `day1.jl` file.
AoC helpfully provides us with a small test example in the project description. Let's make that a constant in the code:

@@juliacode
```plaintext
const TEST_STRING = """1721
979
366
299
675
1456"""
```
@@

We can now write a test set using the `@testset` macro, that makes sure our code works for the test data at least:

@@juliacode
```plaintext
@testset "sum_elements" begin
	@test sum_elements([1, 2, 4], 5) == (1, 4)
	@test sum_elements([0x02, 0x04, 0x07], 0x0b) === (0x04, 0x07)
	@test sum_elements([1], 1) === nothing
	@test sum_elements([6, 4, 1], 8) === nothing
	@test sum_elements(Int[], 0) === nothing

	numbers = [parse(Int, line) for line in eachline(IOBuffer(TEST_STRING))]
	@test sum_elements(numbers, 2020) == (1721, 299)
end
```
@@

To test it, we import our module `AoC2021`, and run `AoC2021.runtests()`:

@@juliacode
```plaintext
julia> using AoC2021.jl

julia> AoC2021.runtests()
                       Pass  
Main.AoC2021.day1:
  sum_elements     |      6
```
@@

## Adding static analysis
After having tested, we can be fairly sure it works as we expect. However, writing a comprehensive test suite that covers all the edge cases is difficult. To be a little more confident our program is well-behaved, we can analyze the behaviour statically.

For this, I use `JET.jl`. We don't need it as part of the `AoC2021` project, so I install the package to the default environment instead:

@@juliacode
```plaintext
(AoC2021) pkg> activate
  Activating environment at `~/.julia/environments/v1.6/Project.toml`

(@v1.6) pkg> add JET
  [ output elided ]

(@v1.6) pkg> activate .
  Activating environment at `~/code/AoC2021/Project.toml`
```
@@

An interesting wrinkle when doing static analysis of Julia is that the program's behaviour is essentially un-analyzable until it is actually compiled, and it is not compiled until we run it - or at least give concrete input types to its functions. Therefore, it is mostly meaningless to try to analyze the _source code_ of our file - instead, we have to analyze specific _uses_ of the code.

Here, for example, I analyze the main function:

@@juliacode
```plaintext
julia> using JET

julia> @report_call AoC2021.day1.solve(IOBuffer())
No errors !
Union{Nothing, Int64}
```
@@

No errors! Whew!

It would be possible to add `JET.jl` static analysis to the test suite. However, with the current state of static analysis in Julia, this is not advisable. Unlike static languages, the presence of a _potential_ error - for example an unresolvable function call - may not be an actual problem in Julia code.
For example, suppose we had a few dependencies, and the dependencies got updated. It would be completely fair game for these dependencies to now include code that is not statically inferrable, but which behaves correctly when running.
It is therefore best to not include static analysis in automatic tests. Static analysis may be improved in future versions of Julia, but for now, we'll leave it here.

## Benchmarking
Wise programmers say that, most of the time, slow code is fast enough, and developers should optimize their code for maintainability instead of speed. That may be true, but that has never stopped me from overengineering otherwise uncomplicated software until it goes _brrrrrr_.
I made another post (TODO: ADD LINK TO OTHER POST) about optimizing Julia code: I won't repeat myself here too much.

We'll use the `BenchmarkTools` package, so let's install it:

@@juliacode
```plaintext
(AoC2021) pkg> add BenchmarkTools
```
@@

Now we can benchmark a function call like so:

@@juliacode
```plaintext
julia> @benchmark open(AoC2021.day1.solve, "data/day1.txt")
BenchmarkTools.Trial: 1609 samples with 1 evaluation.
 Range (min … max):  2.843 ms …   6.682 ms  ┊ GC (min … max): 0.00% … 0.00%
 Time  (median):     2.880 ms               ┊ GC (median):    0.00%
 Time  (mean ± σ):   3.101 ms ± 525.799 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%

  ██                                                        ▃
  ████▇█▇▇▇▇▇▇▆▇▆▆▇▆▇▆▆▆▅▇▅▆▆▆▄▅▆▆▄▆▅▆▅▅▆▄▆▄▆▅▅▅▅▄▅▅▅▅▄▅▄▆▁▆█ █
  2.84 ms      Histogram: log(frequency) by time      4.79 ms <

 Memory estimate: 16.16 KiB, allocs estimate: 420.
```
@@

Those times include reading and parsing the input file. Not too unreasonable!
